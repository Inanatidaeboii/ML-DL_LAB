{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aeb9281",
   "metadata": {},
   "source": [
    "лаборатория 2: Medium - Finding the Optimal 'k'\n",
    "วัตถุประสงค์: ทำความเข้าใจผลกระทบของ Hyperparameter 'k' ในโมเดล k-NN และฝึกฝนกระบวนการหาค่าที่เหมาะสมที่สุด (Hyperparameter Tuning)\n",
    "\n",
    "สิ่งที่ต้องทำ (Tasks):\n",
    "\n",
    "ใช้ข้อมูลที่เตรียมไว้แล้วจาก Lab 1\n",
    "\n",
    "เขียน for loop เพื่อเทรนและประเมินผล KNeighborsClassifier สำหรับค่า k ที่แตกต่างกันไป (เช่น วนตั้งแต่ k=1 ถึง k=30)\n",
    "\n",
    "เก็บค่า Accuracy ของแต่ละ k ไว้ใน List\n",
    "\n",
    "ใช้ Matplotlib พล็อตกราฟระหว่างค่า k (แกน X) กับ Accuracy (แกน Y)\n",
    "\n",
    "จากกราฟ ให้ระบุว่าค่า k ใดให้ Accuracy สูงที่สุด และอภิปรายสั้นๆ ว่าเกิดอะไรขึ้นกับกราฟเมื่อ k มีค่าน้อยมากๆ (อาจเกิด Overfitting) และมากเกินไป (อาจเกิด Underfitting)\n",
    "\n",
    "เกณฑ์การประเมินผล: สร้างกราฟเปรียบเทียบประสิทธิภาพได้ถูกต้อง สามารถระบุค่า k ที่ดีที่สุด และอธิบายปรากฏการณ์ที่สังเกตเห็นจากกราฟได้ โดยเชื่อมโยงกับทฤษฎี Overfitting/Underfitting\n",
    "\n",
    "лаборатория 3: Hard - Logistic Regression from Scratch\n",
    "วัตถุประสงค์: สร้างโมเดล Logistic Regression ด้วย NumPy ตั้งแต่เริ่มต้น เพื่อให้เข้าใจกลไกทางคณิตศาสตร์เบื้องหลัง (Sigmoid, Cost Function, Gradient Descent) อย่างแท้จริง\n",
    "\n",
    "สิ่งที่ต้องทำ (Tasks):\n",
    "\n",
    "ใช้ข้อมูลที่เตรียมไว้แล้วจาก Lab 1\n",
    "\n",
    "เขียนฟังก์ชัน sigmoid(z)\n",
    "\n",
    "เขียนฟังก์ชัน cost_function(y_true, y_pred) เพื่อคำนวณ Binary Cross-Entropy (BCE) Loss\n",
    "\n",
    "เขียนฟังก์ชัน fit(X, y, learning_rate, iterations) ซึ่งภายในจะประกอบด้วย:\n",
    "\n",
    "การเริ่มต้นค่า weights และ bias ด้วยศูนย์\n",
    "\n",
    "Training loop ที่จะทำการคำนวณ Gradient (dw, db) และอัปเดตค่า weights และ bias ในทุกๆรอบ (Gradient Descent)\n",
    "\n",
    "เขียนฟังก์ชัน predict(X) ที่รับข้อมูลเข้ามา, คำนวณความน่าจะเป็น, และ return คลาส (0 หรือ 1) โดยใช้ threshold 0.5\n",
    "\n",
    "นำฟังก์ชันทั้งหมดมาประกอบกันเพื่อเทรนโมเดลบน Training set และวัดค่า Accuracy บน Test set เพื่อเปรียบเทียบกับเวอร์ชันของ scikit-learn\n",
    "\n",
    "เกณฑ์การประเมินผล: สามารถเขียนทุกฟังก์ชันได้ถูกต้องตามหลักคณิตศาสตร์ โมเดลสามารถเรียนรู้และให้ค่า Accuracy บน Test set ได้ใกล้เคียงกับ scikit-learn ซึ่งแสดงถึงความเข้าใจในอัลกอริทึมอย่างลึกซึ้ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c3c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6be614b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "df = pd.DataFrame(data=cancer_data.data, columns=cancer_data.feature_names)\n",
    "df['target'] = cancer_data.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5091a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e632ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3c4b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is : 0.9736842105263158\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.98      0.95      0.96        43\n",
      "     Class 1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score is : {accuracy}')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cff183e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is : 0.956140350877193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       1.00      0.88      0.94        43\n",
      "     Class 1       0.93      1.00      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.94      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_nearest = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "k_nearest.fit(X_train,y_train)\n",
    "y_pred = k_nearest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score is : {accuracy}')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42839c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is : 0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.93      0.93        43\n",
      "     Class 1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_nearest = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "k_nearest.fit(X_train_scaled,y_train)\n",
    "y_pred = k_nearest.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy score is : {accuracy}')\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2bf7cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "neighbor = 30\n",
    "accuracy_list = []\n",
    "for i in range(neighbor):\n",
    "    k_nearest = KNeighborsClassifier(n_neighbors=i+1)\n",
    "    k_nearest.fit(X_train_scaled,y_train)\n",
    "    y_pred = k_nearest.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "\n",
    "plt.plot(list(range(len(accuracy_list))), accuracy_list)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29cd4610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "max_k = 0\n",
    "temp = 0\n",
    "for i in range(len(accuracy_list)):\n",
    "    if accuracy_list[i] > temp:\n",
    "        temp = accuracy_list[i]\n",
    "        max_k = i + 1\n",
    "print(max_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad063aa4",
   "metadata": {},
   "source": [
    "Logistic Regression from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33cbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 0.6931471805599453\n",
      "Loss : 0.6739353141133738\n",
      "Loss : 0.6559064972215113\n",
      "Loss : 0.6389797634212732\n",
      "Loss : 0.623076253001436\n",
      "Loss : 0.6081203882106933\n",
      "Loss : 0.5940406887627786\n",
      "Loss : 0.5807702594879951\n",
      "Loss : 0.5682470068174842\n",
      "Loss : 0.5564136485513381\n",
      "Loss : 0.5452175776527391\n",
      "Loss : 0.5346106312203113\n",
      "Loss : 0.5245488044352469\n",
      "Loss : 0.5149919385754075\n",
      "Loss : 0.5059034032266078\n",
      "Loss : 0.4972497858633656\n",
      "Loss : 0.4890005968533988\n",
      "Loss : 0.48112799433302744\n",
      "Loss : 0.47360653095283395\n",
      "Loss : 0.4664129228944205\n",
      "Loss : 0.4595258405616846\n",
      "Loss : 0.45292571976615165\n",
      "Loss : 0.4465945919192689\n",
      "Loss : 0.440515931618833\n",
      "Loss : 0.4346745200052926\n",
      "Loss : 0.42905632232132845\n",
      "Loss : 0.42364837820475126\n",
      "Loss : 0.4184387033606709\n",
      "Loss : 0.41341620138152296\n",
      "Loss : 0.4085705846051269\n",
      "Loss : 0.4038923030169125\n",
      "Loss : 0.39937248031033273\n",
      "Loss : 0.39500285631813753\n",
      "Loss : 0.39077573511634334\n",
      "Loss : 0.3866839381826414\n",
      "Loss : 0.3827207620621547\n",
      "Loss : 0.3788799400565725\n",
      "Loss : 0.37515560750850196\n",
      "Loss : 0.37154227030211223\n",
      "Loss : 0.3680347762445348\n",
      "Loss : 0.3646282890306759\n",
      "Loss : 0.3613182645277169\n",
      "Loss : 0.3581004291451611\n",
      "Loss : 0.35497076008233086\n",
      "Loss : 0.3519254672681577\n",
      "Loss : 0.34896097682832666\n",
      "Loss : 0.3460739159326661\n",
      "Loss : 0.3432610988914189\n",
      "Loss : 0.340519514382946\n",
      "Loss : 0.337846313707716\n",
      "Loss : 0.3352387999743419\n",
      "Loss : 0.33269441813308454\n",
      "Loss : 0.3302107457808282\n",
      "Loss : 0.32778548466915375\n",
      "Loss : 0.32541645285392373\n",
      "Loss : 0.3231015774308381\n",
      "Loss : 0.3208388878068089\n",
      "Loss : 0.31862650946182125\n",
      "Loss : 0.31646265816025215\n",
      "Loss : 0.3143456345744729\n",
      "Loss : 0.3122738192870158\n",
      "Loss : 0.31024566814068777\n",
      "Loss : 0.3082597079087953\n",
      "Loss : 0.3063145322601543\n",
      "Loss : 0.30440879799580806\n",
      "Loss : 0.30254122153641727\n",
      "Loss : 0.3007105756411139\n",
      "Loss : 0.29891568634027094\n",
      "Loss : 0.297155430066141\n",
      "Loss : 0.29542873096666944\n",
      "Loss : 0.29373455838902224\n",
      "Loss : 0.2920719245204794\n",
      "Loss : 0.29043988217536215\n",
      "Loss : 0.28883752271758\n",
      "Loss : 0.2872639741092231\n",
      "Loss : 0.2857183990763871\n",
      "Loss : 0.2841999933841132\n",
      "Loss : 0.28270798421296245\n",
      "Loss : 0.2812416286303182\n",
      "Loss : 0.27980021215004636\n",
      "Loss : 0.2783830473746207\n",
      "Loss : 0.27698947271427016\n",
      "Loss : 0.27561885117810553\n",
      "Loss : 0.27427056923256143\n",
      "Loss : 0.2729440357228255\n",
      "Loss : 0.27163868085324555\n",
      "Loss : 0.2703539552229924\n",
      "Loss : 0.26908932891351967\n",
      "Loss : 0.26784429062461085\n",
      "Loss : 0.2666183468560244\n",
      "Loss : 0.26541102113195925\n",
      "Loss : 0.2642218532657512\n",
      "Loss : 0.2630503986623913\n",
      "Loss : 0.26189622765661685\n",
      "Loss : 0.26075892488448027\n",
      "Loss : 0.2596380886864382\n",
      "Loss : 0.25853333054013417\n",
      "Loss : 0.257444274521168\n",
      "Loss : 0.2563705567902539\n",
      "Loss : 0.2553118251052768\n",
      "Loss : 0.25426773835684746\n",
      "Loss : 0.2532379661260481\n",
      "Loss : 0.252222188263143\n",
      "Loss : 0.25122009448610383\n",
      "Loss : 0.25023138399787215\n",
      "Loss : 0.24925576512134726\n",
      "Loss : 0.2482929549511499\n",
      "Loss : 0.2473426790212688\n",
      "Loss : 0.2464046709877533\n",
      "Loss : 0.2454786723256618\n",
      "Loss : 0.24456443203952677\n",
      "Loss : 0.24366170638663634\n",
      "Loss : 0.24277025861247745\n",
      "Loss : 0.24188985869772073\n",
      "Loss : 0.2410202831161642\n",
      "Loss : 0.2401613146030864\n",
      "Loss : 0.23931274193349017\n",
      "Loss : 0.23847435970974784\n",
      "Loss : 0.23764596815818684\n",
      "Loss : 0.23682737293417885\n",
      "Loss : 0.23601838493532074\n",
      "Loss : 0.23521882012231832\n",
      "Loss : 0.23442849934720517\n",
      "Loss : 0.23364724818854704\n",
      "Loss : 0.2328748967933045\n",
      "Loss : 0.2321112797250401\n",
      "Loss : 0.2313562358181762\n",
      "Loss : 0.23060960803802355\n",
      "Loss : 0.22987124334631515\n",
      "Loss : 0.22914099257199494\n",
      "Loss : 0.2284187102870227\n",
      "Loss : 0.22770425468696945\n",
      "Loss : 0.22699748747618917\n",
      "Loss : 0.2262982737573627\n",
      "Loss : 0.22560648192522165\n",
      "Loss : 0.22492198356426762\n",
      "Loss : 0.22424465335031368\n",
      "Loss : 0.2235743689556809\n",
      "Loss : 0.2229110109578937\n",
      "Loss : 0.2222544627517234\n",
      "Loss : 0.2216046104644373\n",
      "Loss : 0.22096134287411837\n",
      "Loss : 0.2203245513309255\n",
      "Loss : 0.2196941296811718\n",
      "Loss : 0.2190699741941037\n",
      "Loss : 0.21845198349126949\n",
      "Loss : 0.21784005847837\n",
      "Loss : 0.21723410227949128\n",
      "Loss : 0.2166340201736217\n",
      "Loss : 0.2160397195333614\n",
      "Loss : 0.21545110976573612\n",
      "Loss : 0.21486810225503133\n",
      "Loss : 0.21429061030756666\n",
      "Loss : 0.21371854909833388\n",
      "Loss : 0.2131518356194251\n",
      "Loss : 0.21259038863018173\n",
      "Loss : 0.21203412860899704\n",
      "Loss : 0.2114829777067085\n",
      "Loss : 0.2109368597015194\n",
      "Loss : 0.21039569995539034\n",
      "Loss : 0.20985942537184576\n",
      "Loss : 0.20932796435514167\n",
      "Loss : 0.20880124677074274\n",
      "Loss : 0.20827920390706156\n",
      "Loss : 0.20776176843841115\n",
      "Loss : 0.20724887438912717\n",
      "Loss : 0.2067404570988162\n",
      "Loss : 0.2062364531886894\n",
      "Loss : 0.20573680052894147\n",
      "Loss : 0.20524143820713722\n",
      "Loss : 0.20475030649756956\n",
      "Loss : 0.2042633468315546\n",
      "Loss : 0.2037805017686288\n",
      "Loss : 0.20330171496861837\n",
      "Loss : 0.20282693116454817\n",
      "Loss : 0.20235609613636205\n",
      "Loss : 0.20188915668542515\n",
      "Loss : 0.20142606060978208\n",
      "Loss : 0.20096675668014358\n",
      "Loss : 0.2005111946165778\n",
      "Loss : 0.2000593250658812\n",
      "Loss : 0.19961109957960618\n",
      "Loss : 0.19916647059272297\n",
      "Loss : 0.1987253914028953\n",
      "Loss : 0.19828781615034732\n",
      "Loss : 0.19785369979830428\n",
      "Loss : 0.19742299811398573\n",
      "Loss : 0.1969956676501347\n",
      "Loss : 0.19657166572706405\n",
      "Loss : 0.19615095041520364\n",
      "Loss : 0.1957334805181312\n",
      "Loss : 0.19531921555607248\n",
      "Loss : 0.194908115749854\n",
      "Loss : 0.19450014200529503\n",
      "Loss : 0.19409525589802396\n",
      "Loss : 0.19369341965870565\n",
      "Loss : 0.1932945961586676\n",
      "Loss : 0.19289874889591033\n",
      "Loss : 0.19250584198149243\n",
      "Loss : 0.19211584012627625\n",
      "Loss : 0.19172870862802444\n",
      "Loss : 0.1913444133588359\n",
      "Loss : 0.19096292075291066\n",
      "Loss : 0.1905841977946337\n",
      "Loss : 0.19020821200696783\n",
      "Loss : 0.18983493144014624\n",
      "Loss : 0.1894643246606556\n",
      "Loss : 0.18909636074050126\n",
      "Loss : 0.1887310092467451\n",
      "Loss : 0.18836824023130938\n",
      "Loss : 0.18800802422103693\n",
      "Loss : 0.18765033220800176\n",
      "Loss : 0.18729513564006103\n",
      "Loss : 0.1869424064116431\n",
      "Loss : 0.18659211685476243\n",
      "Loss : 0.1862442397302575\n",
      "Loss : 0.18589874821924254\n",
      "Loss : 0.18555561591476902\n",
      "Loss : 0.18521481681368923\n",
      "Loss : 0.18487632530871764\n",
      "Loss : 0.18454011618068306\n",
      "Loss : 0.18420616459096695\n",
      "Loss : 0.18387444607412268\n",
      "Loss : 0.18354493653066967\n",
      "Loss : 0.18321761222005917\n",
      "Loss : 0.18289244975380486\n",
      "Loss : 0.18256942608877577\n",
      "Loss : 0.18224851852064491\n",
      "Loss : 0.18192970467749142\n",
      "Loss : 0.18161296251354986\n",
      "Loss : 0.18129827030310502\n",
      "Loss : 0.1809856066345256\n",
      "Loss : 0.1806749504044354\n",
      "Loss : 0.18036628081201686\n",
      "Loss : 0.1800595773534436\n",
      "Loss : 0.1797548198164393\n",
      "Loss : 0.17945198827495798\n",
      "Loss : 0.17915106308398443\n",
      "Loss : 0.17885202487444996\n",
      "Loss : 0.17855485454826087\n",
      "Loss : 0.17825953327343755\n",
      "Loss : 0.17796604247935996\n",
      "Loss : 0.17767436385211752\n",
      "Loss : 0.17738447932996088\n",
      "Loss : 0.17709637109885196\n",
      "Loss : 0.17681002158811102\n",
      "Loss : 0.17652541346615705\n",
      "Loss : 0.17624252963634005\n",
      "Loss : 0.1759613532328623\n",
      "Loss : 0.17568186761678642\n",
      "Loss : 0.17540405637212836\n",
      "Loss : 0.17512790330203273\n",
      "Loss : 0.17485339242502873\n",
      "Loss : 0.17458050797136468\n",
      "Loss : 0.17430923437941878\n",
      "Loss : 0.17403955629218482\n",
      "Loss : 0.1737714585538304\n",
      "Loss : 0.17350492620632654\n",
      "Loss : 0.1732399444861461\n",
      "Loss : 0.17297649882103\n",
      "Loss : 0.1727145748268196\n",
      "Loss : 0.17245415830435273\n",
      "Loss : 0.17219523523642352\n",
      "Loss : 0.17193779178480237\n",
      "Loss : 0.1716818142873168\n",
      "Loss : 0.17142728925498976\n",
      "Loss : 0.17117420336923558\n",
      "Loss : 0.1709225434791114\n",
      "Loss : 0.17067229659862268\n",
      "Loss : 0.17042344990408206\n",
      "Loss : 0.17017599073152007\n",
      "Loss : 0.16992990657414603\n",
      "Loss : 0.16968518507985902\n",
      "Loss : 0.1694418140488065\n",
      "Loss : 0.16919978143099068\n",
      "Loss : 0.1689590753239204\n",
      "Loss : 0.1687196839703088\n",
      "Loss : 0.1684815957558139\n",
      "Loss : 0.1682447992068236\n",
      "Loss : 0.16800928298828152\n",
      "Loss : 0.16777503590155443\n",
      "Loss : 0.16754204688233995\n",
      "Loss : 0.1673103049986132\n",
      "Loss : 0.16707979944861223\n",
      "Loss : 0.16685051955886052\n",
      "Loss : 0.16662245478222665\n",
      "Loss : 0.16639559469601953\n",
      "Loss : 0.16616992900011907\n",
      "Loss : 0.16594544751514065\n",
      "Loss : 0.1657221401806339\n",
      "Loss : 0.1654999970533137\n",
      "Loss : 0.16527900830532377\n",
      "Loss : 0.16505916422253133\n",
      "Loss : 0.16484045520285304\n",
      "Loss : 0.1646228717546104\n",
      "Loss : 0.1644064044949153\n",
      "Loss : 0.16419104414808391\n",
      "Loss : 0.16397678154407908\n",
      "Loss : 0.16376360761698022\n",
      "Loss : 0.16355151340348043\n",
      "Loss : 0.16334049004140977\n",
      "Loss : 0.16313052876828485\n",
      "Loss : 0.16292162091988371\n",
      "Loss : 0.1627137579288452\n",
      "Loss : 0.1625069313232933\n",
      "Loss : 0.16230113272548485\n",
      "Loss : 0.1620963538504808\n",
      "Loss : 0.16189258650484017\n",
      "Loss : 0.16168982258533673\n",
      "Loss : 0.16148805407769753\n",
      "Loss : 0.16128727305536253\n",
      "Loss : 0.16108747167826598\n",
      "Loss : 0.16088864219163776\n",
      "Loss : 0.16069077692482564\n",
      "Loss : 0.16049386829013676\n",
      "Loss : 0.1602979087816989\n",
      "Loss : 0.16010289097434063\n",
      "Loss : 0.15990880752249018\n",
      "Loss : 0.15971565115909275\n",
      "Loss : 0.1595234146945454\n",
      "Loss : 0.15933209101564985\n",
      "Loss : 0.1591416730845824\n",
      "Loss : 0.15895215393788076\n",
      "Loss : 0.15876352668544744\n",
      "Loss : 0.15857578450956936\n",
      "Loss : 0.1583889206639536\n",
      "Loss : 0.15820292847277834\n",
      "Loss : 0.1580178013297598\n",
      "Loss : 0.15783353269723338\n",
      "Loss : 0.1576501161052504\n",
      "Loss : 0.15746754515068875\n",
      "Loss : 0.15728581349637782\n",
      "Loss : 0.15710491487023756\n",
      "Loss : 0.1569248430644311\n",
      "Loss : 0.15674559193453055\n",
      "Loss : 0.15656715539869648\n",
      "Loss : 0.1563895274368696\n",
      "Loss : 0.15621270208997595\n",
      "Loss : 0.15603667345914365\n",
      "Loss : 0.1558614357049326\n",
      "Loss : 0.1556869830465758\n",
      "Loss : 0.15551330976123212\n",
      "Loss : 0.15534041018325156\n",
      "Loss : 0.15516827870345082\n",
      "Loss : 0.15499690976840053\n",
      "Loss : 0.15482629787972324\n",
      "Loss : 0.15465643759340245\n",
      "Loss : 0.15448732351910152\n",
      "Loss : 0.1543189503194937\n",
      "Loss : 0.1541513127096018\n",
      "Loss : 0.1539844054561478\n",
      "Loss : 0.15381822337691264\n",
      "Loss : 0.1536527613401054\n",
      "Loss : 0.15348801426374178\n",
      "Loss : 0.15332397711503237\n",
      "Loss : 0.15316064490977932\n",
      "Loss : 0.15299801271178265\n",
      "Loss : 0.1528360756322548\n",
      "Loss : 0.1526748288292444\n",
      "Loss : 0.15251426750706812\n",
      "Loss : 0.15235438691575076\n",
      "Loss : 0.1521951823504742\n",
      "Loss : 0.1520366491510335\n",
      "Loss : 0.15187878270130176\n",
      "Loss : 0.15172157842870232\n",
      "Loss : 0.15156503180368877\n",
      "Loss : 0.15140913833923209\n",
      "Loss : 0.15125389359031596\n",
      "Loss : 0.15109929315343856\n",
      "Loss : 0.1509453326661222\n",
      "Loss : 0.15079200780642912\n",
      "Loss : 0.1506393142924851\n",
      "Loss : 0.15048724788200935\n",
      "Loss : 0.15033580437185118\n",
      "Loss : 0.1501849795975332\n",
      "Loss : 0.15003476943280097\n",
      "Loss : 0.14988516978917912\n",
      "Loss : 0.14973617661553368\n",
      "Loss : 0.14958778589764032\n",
      "Loss : 0.14943999365775903\n",
      "Loss : 0.14929279595421416\n",
      "Loss : 0.14914618888098116\n",
      "Loss : 0.14900016856727794\n",
      "Loss : 0.14885473117716305\n",
      "Loss : 0.14870987290913848\n",
      "Loss : 0.14856558999575856\n",
      "Loss : 0.14842187870324383\n",
      "Loss : 0.14827873533110034\n",
      "Loss : 0.14813615621174425\n",
      "Loss : 0.14799413771013153\n",
      "Loss : 0.1478526762233925\n",
      "Loss : 0.1477117681804715\n",
      "Loss : 0.1475714100417716\n",
      "Loss : 0.14743159829880367\n",
      "Loss : 0.14729232947384066\n",
      "Loss : 0.1471536001195761\n",
      "Loss : 0.14701540681878744\n",
      "Loss : 0.14687774618400404\n",
      "Loss : 0.14674061485717907\n",
      "Loss : 0.14660400950936636\n",
      "Loss : 0.1464679268404012\n",
      "Loss : 0.14633236357858526\n",
      "Loss : 0.14619731648037623\n",
      "Loss : 0.14606278233008088\n",
      "Loss : 0.1459287579395528\n",
      "Loss : 0.14579524014789327\n",
      "Loss : 0.14566222582115718\n",
      "Loss : 0.1455297118520616\n",
      "Loss : 0.1453976951596992\n",
      "Loss : 0.14526617268925457\n",
      "Loss : 0.1451351414117248\n",
      "Loss : 0.1450045983236432\n",
      "Loss : 0.1448745404468073\n",
      "Loss : 0.1447449648280093\n",
      "Loss : 0.14461586853877104\n",
      "Loss : 0.14448724867508161\n",
      "Loss : 0.14435910235713897\n",
      "Loss : 0.1442314267290941\n",
      "Loss : 0.1441042189587992\n",
      "Loss : 0.1439774762375585\n",
      "Loss : 0.14385119577988253\n",
      "Loss : 0.14372537482324546\n",
      "Loss : 0.1436000106278454\n",
      "Loss : 0.14347510047636777\n",
      "Loss : 0.14335064167375186\n",
      "Loss : 0.14322663154695986\n",
      "Loss : 0.1431030674447492\n",
      "Loss : 0.14297994673744768\n",
      "Loss : 0.14285726681673097\n",
      "Loss : 0.1427350250954036\n",
      "Loss : 0.142613219007182\n",
      "Loss : 0.14249184600648063\n",
      "Loss : 0.1423709035682007\n",
      "Loss : 0.1422503891875212\n",
      "Loss : 0.14213030037969285\n",
      "Loss : 0.1420106346798346\n",
      "Loss : 0.1418913896427322\n",
      "Loss : 0.14177256284263964\n",
      "Loss : 0.141654151873083\n",
      "Loss : 0.14153615434666628\n",
      "Loss : 0.1414185678948802\n",
      "Loss : 0.1413013901679127\n",
      "Loss : 0.14118461883446234\n",
      "Loss : 0.14106825158155328\n",
      "Loss : 0.1409522861143531\n",
      "Loss : 0.14083672015599258\n",
      "Loss : 0.14072155144738727\n",
      "Loss : 0.14060677774706204\n",
      "Loss : 0.14049239683097683\n",
      "Loss : 0.14037840649235503\n",
      "Loss : 0.14026480454151383\n",
      "Loss : 0.14015158880569625\n",
      "Loss : 0.14003875712890568\n",
      "Loss : 0.13992630737174197\n",
      "Loss : 0.13981423741123947\n",
      "Loss : 0.13970254514070737\n",
      "Loss : 0.13959122846957142\n",
      "Loss : 0.1394802853232178\n",
      "Loss : 0.13936971364283873\n",
      "Loss : 0.13925951138527992\n",
      "Loss : 0.1391496765228897\n",
      "Loss : 0.13904020704337008\n",
      "Loss : 0.13893110094962932\n",
      "Loss : 0.13882235625963643\n",
      "Loss : 0.13871397100627714\n",
      "Loss : 0.13860594323721173\n",
      "Loss : 0.13849827101473433\n",
      "Loss : 0.13839095241563387\n",
      "Loss : 0.1382839855310569\n",
      "Loss : 0.13817736846637135\n",
      "Loss : 0.13807109934103265\n",
      "Loss : 0.13796517628845054\n",
      "Loss : 0.13785959745585827\n",
      "Loss : 0.13775436100418248\n",
      "Loss : 0.13764946510791515\n",
      "Loss : 0.13754490795498645\n",
      "Loss : 0.13744068774663973\n",
      "Loss : 0.13733680269730722\n",
      "Loss : 0.1372332510344875\n",
      "Loss : 0.1371300309986243\n",
      "Loss : 0.13702714084298673\n",
      "Loss : 0.13692457883355053\n",
      "Loss : 0.13682234324888098\n",
      "Loss : 0.13672043238001705\n",
      "Loss : 0.13661884453035666\n",
      "Loss : 0.13651757801554346\n",
      "Loss : 0.1364166311633547\n",
      "Loss : 0.1363160023135903\n",
      "Loss : 0.13621568981796334\n",
      "Loss : 0.1361156920399915\n",
      "Loss : 0.13601600735489\n",
      "Loss : 0.1359166341494655\n",
      "Loss : 0.13581757082201104\n",
      "Loss : 0.13571881578220252\n",
      "Loss : 0.13562036745099598\n",
      "Loss : 0.13552222426052604\n",
      "Loss : 0.13542438465400558\n",
      "Loss : 0.13532684708562653\n",
      "Loss : 0.1352296100204614\n",
      "Loss : 0.13513267193436632\n",
      "Loss : 0.13503603131388472\n",
      "Loss : 0.13493968665615258\n",
      "Loss : 0.13484363646880385\n",
      "Loss : 0.13474787926987786\n",
      "Loss : 0.134652413587727\n",
      "Loss : 0.13455723796092578\n",
      "Loss : 0.13446235093818054\n",
      "Loss : 0.1343677510782405\n",
      "Loss : 0.13427343694980948\n",
      "Loss : 0.13417940713145854\n",
      "Loss : 0.13408566021153975\n",
      "Loss : 0.13399219478810082\n",
      "Loss : 0.13389900946880032\n",
      "Loss : 0.13380610287082428\n",
      "Loss : 0.13371347362080338\n",
      "Loss : 0.133621120354731\n",
      "Loss : 0.13352904171788219\n",
      "Loss : 0.13343723636473345\n",
      "Loss : 0.13334570295888348\n",
      "Loss : 0.1332544401729747\n",
      "Loss : 0.1331634466886152\n",
      "Loss : 0.13307272119630234\n",
      "Loss : 0.1329822623953462\n",
      "Loss : 0.13289206899379458\n",
      "Loss : 0.13280213970835816\n",
      "Loss : 0.13271247326433708\n",
      "Loss : 0.13262306839554758\n",
      "Loss : 0.13253392384425\n",
      "Loss : 0.13244503836107707\n",
      "Loss : 0.1323564107049632\n",
      "Loss : 0.1322680396430745\n",
      "Loss : 0.13217992395073916\n",
      "Loss : 0.13209206241137914\n",
      "Loss : 0.13200445381644196\n",
      "Loss : 0.1319170969653335\n",
      "Loss : 0.13182999066535148\n",
      "Loss : 0.13174313373161955\n",
      "Loss : 0.13165652498702196\n",
      "Loss : 0.13157016326213905\n",
      "Loss : 0.13148404739518327\n",
      "Loss : 0.1313981762319359\n",
      "Loss : 0.1313125486256844\n",
      "Loss : 0.13122716343716043\n",
      "Loss : 0.13114201953447818\n",
      "Loss : 0.131057115793074\n",
      "Loss : 0.1309724510956458\n",
      "Loss : 0.13088802433209365\n",
      "Loss : 0.13080383439946075\n",
      "Loss : 0.130719880201875\n",
      "Loss : 0.13063616065049105\n",
      "Loss : 0.13055267466343323\n",
      "Loss : 0.13046942116573865\n",
      "Loss : 0.1303863990893011\n",
      "Loss : 0.1303036073728156\n",
      "Loss : 0.130221044961723\n",
      "Loss : 0.13013871080815578\n",
      "Loss : 0.130056603870884\n",
      "Loss : 0.12997472311526168\n",
      "Loss : 0.129893067513174\n",
      "Loss : 0.12981163604298487\n",
      "Loss : 0.129730427689485\n",
      "Loss : 0.12964944144384036\n",
      "Loss : 0.1295686763035415\n",
      "Loss : 0.12948813127235295\n",
      "Loss : 0.12940780536026328\n",
      "Loss : 0.12932769758343576\n",
      "Loss : 0.12924780696415927\n",
      "Loss : 0.1291681325307998\n",
      "Loss : 0.1290886733177525\n",
      "Loss : 0.12900942836539386\n",
      "Loss : 0.12893039672003487\n",
      "Loss : 0.12885157743387413\n",
      "Loss : 0.12877296956495163\n",
      "Loss : 0.12869457217710303\n",
      "Loss : 0.12861638433991418\n",
      "Loss : 0.12853840512867631\n",
      "Loss : 0.1284606336243414\n",
      "Loss : 0.1283830689134781\n",
      "Loss : 0.12830571008822808\n",
      "Loss : 0.12822855624626284\n",
      "Loss : 0.12815160649074064\n",
      "Loss : 0.12807485993026427\n",
      "Loss : 0.12799831567883876\n",
      "Loss : 0.12792197285582996\n",
      "Loss : 0.12784583058592297\n",
      "Loss : 0.12776988799908148\n",
      "Loss : 0.12769414423050723\n",
      "Loss : 0.1276185984205997\n",
      "Loss : 0.12754324971491643\n",
      "Loss : 0.1274680972641337\n",
      "Loss : 0.12739314022400736\n",
      "Loss : 0.1273183777553343\n",
      "Loss : 0.127243809023914\n",
      "Loss : 0.12716943320051063\n",
      "Loss : 0.12709524946081532\n",
      "Loss : 0.12702125698540906\n",
      "Loss : 0.12694745495972565\n",
      "Loss : 0.12687384257401516\n",
      "Loss : 0.12680041902330758\n",
      "Loss : 0.12672718350737697\n",
      "Loss : 0.1266541352307058\n",
      "Loss : 0.12658127340244976\n",
      "Loss : 0.12650859723640268\n",
      "Loss : 0.12643610595096197\n",
      "Loss : 0.1263637987690942\n",
      "Loss : 0.12629167491830115\n",
      "Loss : 0.12621973363058614\n",
      "Loss : 0.12614797414242046\n",
      "Loss : 0.12607639569471038\n",
      "Loss : 0.12600499753276428\n",
      "Loss : 0.12593377890626023\n",
      "Loss : 0.12586273906921355\n",
      "Loss : 0.12579187727994506\n",
      "Loss : 0.12572119280104935\n",
      "Loss : 0.12565068489936343\n",
      "Loss : 0.12558035284593552\n",
      "Loss : 0.1255101959159944\n",
      "Loss : 0.1254402133889187\n",
      "Loss : 0.12537040454820672\n",
      "Loss : 0.1253007686814464\n",
      "Loss : 0.12523130508028565\n",
      "Loss : 0.12516201304040256\n",
      "Loss : 0.1250928918614766\n",
      "Loss : 0.12502394084715931\n",
      "Loss : 0.12495515930504575\n",
      "Loss : 0.12488654654664605\n",
      "Loss : 0.12481810188735708\n",
      "Loss : 0.12474982464643454\n",
      "Loss : 0.12468171414696529\n",
      "Loss : 0.12461376971583976\n",
      "Loss : 0.12454599068372482\n",
      "Loss : 0.12447837638503663\n",
      "Loss : 0.12441092615791396\n",
      "Loss : 0.12434363934419165\n",
      "Loss : 0.12427651528937428\n",
      "Loss : 0.12420955334261002\n",
      "Loss : 0.12414275285666485\n",
      "Loss : 0.12407611318789687\n",
      "Loss : 0.12400963369623093\n",
      "Loss : 0.12394331374513333\n",
      "Loss : 0.12387715270158692\n",
      "Loss : 0.12381114993606634\n",
      "Loss : 0.12374530482251334\n",
      "Loss : 0.12367961673831254\n",
      "Loss : 0.12361408506426726\n",
      "Loss : 0.12354870918457553\n",
      "Loss : 0.12348348848680638\n",
      "Loss : 0.12341842236187632\n",
      "Loss : 0.12335351020402599\n",
      "Loss : 0.12328875141079701\n",
      "Loss : 0.12322414538300905\n",
      "Loss : 0.12315969152473706\n",
      "Loss : 0.1230953892432887\n",
      "Loss : 0.123031237949182\n",
      "Loss : 0.12296723705612314\n",
      "Loss : 0.12290338598098449\n",
      "Loss : 0.12283968414378281\n",
      "Loss : 0.12277613096765747\n",
      "Loss : 0.12271272587884918\n",
      "Loss : 0.12264946830667868\n",
      "Loss : 0.12258635768352555\n",
      "Loss : 0.12252339344480735\n",
      "Loss : 0.12246057502895888\n",
      "Loss : 0.12239790187741156\n",
      "Loss : 0.12233537343457308\n",
      "Loss : 0.12227298914780715\n",
      "Loss : 0.12221074846741338\n",
      "Loss : 0.12214865084660742\n",
      "Loss : 0.12208669574150115\n",
      "Loss : 0.12202488261108323\n",
      "Loss : 0.12196321091719955\n",
      "Loss : 0.12190168012453399\n",
      "Loss : 0.12184028970058942\n",
      "Loss : 0.1217790391156686\n",
      "Loss : 0.12171792784285555\n",
      "Loss : 0.12165695535799666\n",
      "Loss : 0.12159612113968249\n",
      "Loss : 0.12153542466922927\n",
      "Loss : 0.12147486543066076\n",
      "Loss : 0.12141444291069009\n",
      "Loss : 0.12135415659870205\n",
      "Loss : 0.12129400598673519\n",
      "Loss : 0.12123399056946425\n",
      "Loss : 0.12117410984418278\n",
      "Loss : 0.12111436331078561\n",
      "Loss : 0.12105475047175183\n",
      "Loss : 0.12099527083212773\n",
      "Loss : 0.12093592389950979\n",
      "Loss : 0.12087670918402796\n",
      "Loss : 0.12081762619832905\n",
      "Loss : 0.12075867445756008\n",
      "Loss : 0.12069985347935208\n",
      "Loss : 0.12064116278380371\n",
      "Loss : 0.1205826018934651\n",
      "Loss : 0.12052417033332202\n",
      "Loss : 0.12046586763077992\n",
      "Loss : 0.1204076933156481\n",
      "Loss : 0.12034964692012418\n",
      "Loss : 0.12029172797877868\n",
      "Loss : 0.12023393602853949\n",
      "Loss : 0.1201762706086767\n",
      "Loss : 0.12011873126078744\n",
      "Loss : 0.12006131752878088\n",
      "Loss : 0.12000402895886339\n",
      "Loss : 0.11994686509952357\n",
      "Loss : 0.1198898255015178\n",
      "Loss : 0.11983290971785557\n",
      "Loss : 0.1197761173037851\n",
      "Loss : 0.11971944781677887\n",
      "Loss : 0.11966290081651966\n",
      "Loss : 0.11960647586488626\n",
      "Loss : 0.11955017252593943\n",
      "Loss : 0.11949399036590819\n",
      "Loss : 0.11943792895317588\n",
      "Loss : 0.11938198785826666\n",
      "Loss : 0.11932616665383165\n",
      "Loss : 0.11927046491463585\n",
      "Loss : 0.11921488221754441\n",
      "Loss : 0.11915941814150968\n",
      "Loss : 0.11910407226755777\n",
      "Loss : 0.11904884417877575\n",
      "Loss : 0.11899373346029853\n",
      "Loss : 0.11893873969929598\n",
      "Loss : 0.11888386248496041\n",
      "Loss : 0.11882910140849358\n",
      "Loss : 0.11877445606309434\n",
      "Loss : 0.1187199260439461\n",
      "Loss : 0.1186655109482045\n",
      "Loss : 0.11861121037498505\n",
      "Loss : 0.11855702392535099\n",
      "Loss : 0.11850295120230114\n",
      "Loss : 0.118448991810758\n",
      "Loss : 0.11839514535755571\n",
      "Loss : 0.1183414114514283\n",
      "Loss : 0.11828778970299794\n",
      "Loss : 0.11823427972476325\n",
      "Loss : 0.1181808811310878\n",
      "Loss : 0.11812759353818851\n",
      "Loss : 0.11807441656412447\n",
      "Loss : 0.11802134982878548\n",
      "Loss : 0.11796839295388076\n",
      "Loss : 0.11791554556292805\n",
      "Loss : 0.11786280728124232\n",
      "Loss : 0.11781017773592496\n",
      "Loss : 0.11775765655585277\n",
      "Loss : 0.11770524337166721\n",
      "Loss : 0.11765293781576369\n",
      "Loss : 0.11760073952228083\n",
      "Loss : 0.11754864812709\n",
      "Loss : 0.1174966632677847\n",
      "Loss : 0.11744478458367029\n",
      "Loss : 0.11739301171575349\n",
      "Loss : 0.11734134430673225\n",
      "Loss : 0.11728978200098543\n",
      "Loss : 0.11723832444456282\n",
      "Loss : 0.11718697128517497\n",
      "Loss : 0.11713572217218331\n",
      "Loss : 0.11708457675659023\n",
      "Loss : 0.11703353469102923\n",
      "Loss : 0.1169825956297552\n",
      "Loss : 0.11693175922863468\n",
      "Loss : 0.11688102514513633\n",
      "Loss : 0.11683039303832136\n",
      "Loss : 0.11677986256883403\n",
      "Loss : 0.11672943339889222\n",
      "Loss : 0.11667910519227817\n",
      "Loss : 0.11662887761432919\n",
      "Loss : 0.1165787503319284\n",
      "Loss : 0.11652872301349565\n",
      "Loss : 0.11647879532897842\n",
      "Loss : 0.11642896694984281\n",
      "Loss : 0.11637923754906467\n",
      "Loss : 0.11632960680112063\n",
      "Loss : 0.11628007438197925\n",
      "Loss : 0.11623063996909246\n",
      "Loss : 0.1161813032413867\n",
      "Loss : 0.1161320638792543\n",
      "Loss : 0.11608292156454499\n",
      "Loss : 0.11603387598055738\n",
      "Loss : 0.11598492681203043\n",
      "Loss : 0.1159360737451352\n",
      "Loss : 0.11588731646746642\n",
      "Loss : 0.11583865466803424\n",
      "Loss : 0.11579008803725609\n",
      "Loss : 0.1157416162669484\n",
      "Loss : 0.11569323905031866\n",
      "Loss : 0.11564495608195724\n",
      "Loss : 0.11559676705782954\n",
      "Loss : 0.11554867167526793\n",
      "Loss : 0.11550066963296404\n",
      "Loss : 0.1154527606309608\n",
      "Loss : 0.11540494437064469\n",
      "Loss : 0.11535722055473824\n",
      "Loss : 0.11530958888729205\n",
      "Loss : 0.11526204907367747\n",
      "Loss : 0.11521460082057892\n",
      "Loss : 0.11516724383598644\n",
      "Loss : 0.11511997782918822\n",
      "Loss : 0.11507280251076317\n",
      "Loss : 0.1150257175925738\n",
      "Loss : 0.11497872278775861\n",
      "Loss : 0.1149318178107251\n",
      "Loss : 0.11488500237714243\n",
      "Loss : 0.11483827620393444\n",
      "Loss : 0.11479163900927238\n",
      "Loss : 0.114745090512568\n",
      "Loss : 0.11469863043446647\n",
      "Loss : 0.11465225849683963\n",
      "Loss : 0.11460597442277877\n",
      "Loss : 0.11455977793658804\n",
      "Loss : 0.11451366876377751\n",
      "Loss : 0.11446764663105656\n",
      "Loss : 0.11442171126632705\n",
      "Loss : 0.11437586239867664\n",
      "Loss : 0.1143300997583722\n",
      "Loss : 0.11428442307685334\n",
      "Loss : 0.11423883208672572\n",
      "Loss : 0.11419332652175467\n",
      "Loss : 0.11414790611685868\n",
      "Loss : 0.11410257060810304\n",
      "Loss : 0.11405731973269354\n",
      "Loss : 0.11401215322896996\n",
      "Loss : 0.11396707083640013\n",
      "Loss : 0.11392207229557336\n",
      "Loss : 0.11387715734819445\n",
      "Loss : 0.11383232573707754\n",
      "Loss : 0.1137875772061399\n",
      "Loss : 0.11374291150039592\n",
      "Loss : 0.11369832836595124\n",
      "Loss : 0.11365382754999649\n",
      "Loss : 0.11360940880080159\n",
      "Loss : 0.11356507186770963\n",
      "Loss : 0.11352081650113123\n",
      "Loss : 0.11347664245253856\n",
      "Loss : 0.11343254947445965\n",
      "Loss : 0.11338853732047251\n",
      "Loss : 0.11334460574519963\n",
      "Loss : 0.11330075450430208\n",
      "Loss : 0.11325698335447404\n",
      "Loss : 0.11321329205343718\n",
      "Loss : 0.11316968035993498\n",
      "Loss : 0.11312614803372732\n",
      "Loss : 0.11308269483558508\n",
      "Loss : 0.11303932052728441\n",
      "Loss : 0.11299602487160158\n",
      "Loss : 0.11295280763230749\n",
      "Loss : 0.11290966857416236\n",
      "Loss : 0.11286660746291031\n",
      "Loss : 0.11282362406527428\n",
      "Loss : 0.11278071814895065\n",
      "Loss : 0.11273788948260402\n",
      "Loss : 0.11269513783586219\n",
      "Loss : 0.11265246297931077\n",
      "Loss : 0.11260986468448837\n",
      "Loss : 0.1125673427238813\n",
      "Loss : 0.11252489687091861\n",
      "Loss : 0.11248252689996709\n",
      "Loss : 0.11244023258632634\n",
      "Loss : 0.1123980137062237\n",
      "Loss : 0.11235587003680944\n",
      "Loss : 0.11231380135615188\n",
      "Loss : 0.11227180744323252\n",
      "Loss : 0.1122298880779412\n",
      "Loss : 0.11218804304107133\n",
      "Loss : 0.11214627211431519\n",
      "Loss : 0.1121045750802591\n",
      "Loss : 0.11206295172237882\n",
      "Loss : 0.11202140182503488\n",
      "Loss : 0.11197992517346786\n",
      "Loss : 0.11193852155379383\n",
      "Loss : 0.11189719075299985\n",
      "Loss : 0.11185593255893932\n",
      "Loss : 0.11181474676032752\n",
      "Loss : 0.11177363314673709\n",
      "Loss : 0.11173259150859359\n",
      "Loss : 0.11169162163717106\n",
      "Loss : 0.11165072332458761\n",
      "Loss : 0.11160989636380102\n",
      "Loss : 0.11156914054860449\n",
      "Loss : 0.11152845567362223\n",
      "Loss : 0.11148784153430515\n",
      "Loss : 0.11144729792692668\n",
      "Loss : 0.11140682464857843\n",
      "Loss : 0.11136642149716604\n",
      "Loss : 0.11132608827140503\n",
      "Loss : 0.11128582477081649\n",
      "Loss : 0.1112456307957231\n",
      "Loss : 0.111205506147245\n",
      "Loss : 0.11116545062729562\n",
      "Loss : 0.11112546403857763\n",
      "Loss : 0.11108554618457904\n",
      "Loss : 0.1110456968695691\n",
      "Loss : 0.11100591589859422\n",
      "Loss : 0.11096620307747418\n",
      "Loss : 0.11092655821279811\n",
      "Loss : 0.11088698111192065\n",
      "Loss : 0.11084747158295793\n",
      "Loss : 0.11080802943478385\n",
      "Loss : 0.11076865447702616\n",
      "Loss : 0.11072934652006267\n",
      "Loss : 0.11069010537501743\n",
      "Loss : 0.1106509308537571\n",
      "Loss : 0.11061182276888701\n",
      "Loss : 0.11057278093374753\n",
      "Loss : 0.11053380516241042\n",
      "Loss : 0.11049489526967507\n",
      "Loss : 0.11045605107106488\n",
      "Loss : 0.1104172723828237\n",
      "Loss : 0.11037855902191204\n",
      "Loss : 0.11033991080600376\n",
      "Loss : 0.1103013275534822\n",
      "Loss : 0.11026280908343686\n",
      "Loss : 0.11022435521565978\n",
      "Loss : 0.11018596577064203\n",
      "Loss : 0.11014764056957038\n",
      "Loss : 0.11010937943432354\n",
      "Loss : 0.11007118218746914\n",
      "Loss : 0.11003304865225988\n",
      "Loss : 0.10999497865263044\n",
      "Loss : 0.10995697201319403\n",
      "Loss : 0.10991902855923895\n",
      "Loss : 0.10988114811672532\n",
      "Loss : 0.10984333051228179\n",
      "Loss : 0.10980557557320227\n",
      "Loss : 0.10976788312744246\n",
      "Loss : 0.10973025300361688\n",
      "Loss : 0.10969268503099547\n",
      "Loss : 0.10965517903950033\n",
      "Loss : 0.10961773485970266\n",
      "Loss : 0.10958035232281951\n",
      "Loss : 0.10954303126071058\n",
      "Loss : 0.10950577150587518\n",
      "Loss : 0.10946857289144904\n",
      "Loss : 0.1094314352512012\n",
      "Loss : 0.10939435841953105\n",
      "Loss : 0.10935734223146501\n",
      "Loss : 0.10932038652265373\n",
      "Loss : 0.10928349112936897\n",
      "Loss : 0.10924665588850056\n",
      "Loss : 0.10920988063755342\n",
      "Loss : 0.10917316521464465\n",
      "Loss : 0.10913650945850048\n",
      "Loss : 0.10909991320845336\n",
      "Loss : 0.10906337630443902\n",
      "Loss : 0.10902689858699365\n",
      "Loss : 0.10899047989725093\n",
      "Loss : 0.10895412007693914\n",
      "Loss : 0.10891781896837836\n",
      "Loss : 0.10888157641447756\n",
      "Loss : 0.10884539225873191\n",
      "Loss : 0.10880926634521985\n",
      "Loss : 0.10877319851860033\n",
      "Loss : 0.10873718862411003\n",
      "Loss : 0.10870123650756068\n",
      "Loss : 0.10866534201533616\n",
      "Loss : 0.10862950499438999\n",
      "Loss : 0.10859372529224243\n",
      "Loss : 0.10855800275697786\n",
      "Loss : 0.10852233723724215\n",
      "Loss : 0.10848672858223986\n",
      "Loss : 0.10845117664173178\n",
      "Loss : 0.10841568126603211\n",
      "Loss : 0.108380242306006\n",
      "Loss : 0.10834485961306685\n",
      "Loss : 0.10830953303917377\n",
      "Loss : 0.10827426243682894\n",
      "Loss : 0.10823904765907522\n",
      "Loss : 0.10820388855949332\n",
      "Loss : 0.1081687849921996\n",
      "Loss : 0.10813373681184331\n",
      "Loss : 0.10809874387360423\n",
      "Loss : 0.1080638060331901\n",
      "Loss : 0.10802892314683424\n",
      "Loss : 0.10799409507129293\n",
      "Loss : 0.10795932166384321\n",
      "Loss : 0.10792460278228022\n",
      "Loss : 0.10788993828491497\n",
      "Loss : 0.10785532803057171\n",
      "Loss : 0.10782077187858591\n",
      "Loss : 0.1077862696888014\n",
      "Loss : 0.10775182132156846\n",
      "Loss : 0.10771742663774121\n",
      "Loss : 0.10768308549867539\n",
      "Loss : 0.10764879776622595\n",
      "Loss : 0.10761456330274477\n",
      "Loss : 0.10758038197107851\n",
      "Loss : 0.10754625363456605\n",
      "Loss : 0.10751217815703651\n",
      "Loss : 0.10747815540280675\n",
      "Loss : 0.1074441852366793\n",
      "Loss : 0.10741026752394008\n",
      "Loss : 0.10737640213035614\n",
      "Loss : 0.10734258892217344\n",
      "Loss : 0.10730882776611475\n",
      "Loss : 0.10727511852937745\n",
      "Loss : 0.10724146107963121\n",
      "Loss : 0.10720785528501607\n",
      "Loss : 0.10717430101414013\n",
      "Loss : 0.10714079813607746\n",
      "Loss : 0.10710734652036596\n",
      "0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionFromScratch:\n",
    "    def __init__(self, lr = 0.01, iteration = 1000):\n",
    "        self.lr = lr\n",
    "        self.iteration = iteration\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def cost(self, p, y):\n",
    "        m_samples = len(y)\n",
    "        return -(1/m_samples) * np.sum(y*(np.log(p)) + (1-y)*(np.log(1-p)))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y = np.array(y)\n",
    "        m_samples, n_features = X.shape\n",
    "        self.weight = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        for _ in range(self.iteration):\n",
    "            z = np.dot(X,self.weight) + self.bias\n",
    "            p = self.sigmoid(z)\n",
    "            loss = self.cost(p,y)\n",
    "            self.cost_history.append(loss)\n",
    "            print(f'Loss is : {loss}')\n",
    "            dw = (1/m_samples) * np.dot(X.T, (p-y))\n",
    "            db = (1/m_samples) * np.sum((p-y))\n",
    "            self.weight = self.weight - (self.lr * dw)\n",
    "            self.bias = self.bias - (self.lr * db)\n",
    "\n",
    "    def predict(self,X):\n",
    "        return (self.sigmoid(np.dot(X,self.weight) + self.bias) >= 0.5).astype(int)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logistic_model_scratch = LogisticRegressionFromScratch()\n",
    "    logistic_model_scratch.fit(X_train_scaled,y_train)\n",
    "    y_pred = logistic_model_scratch.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
